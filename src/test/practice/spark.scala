package test.practice

import org.apache.spark.sql.SparkSession
import org.apache.spark._
import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel

object spark {
  case class Customer(name:String,city :String,age:Int)
  def main(args:Array[String]) {
    
    val conf = new SparkConf().setMaster("local[*]").setAppName("local-lab01")
   val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    //val hadooplines= sc.textFile("hdfs:/user/hduser/empdata.txt")
    val lines = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
    val alllines = sc.textFile("file:/home/hduser/sparkdata/*data.txt")
    //hadooplines.foreach(println)
    alllines.foreach(println) //RDD using spark session
    println("Filter conditions for printing lines which is greater then 12 char")
    var filterlines = lines.filter(l=>l.length>12) //RDD from another RDD
    println("Count is " + filterlines.count())
    filterlines.foreach(println)
    filterlines.cache()
    println(filterlines.count())
    //Transformations
    println("Transformation using MAP")
    val lines1 = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
    val length = lines1.map(l=>l.length)
    length.foreach(println)
    val split1 = lines1.map(l=>l.split(",")).filter(l=>l(1).toUpperCase=="CHENNAI")
      split1.collect().foreach(sp=>println(sp.toList)) // if you don use toList or mkstring you will get addres  of the array, since toSting() is not overloaded so it will show hashcode value.
    val fmrdd = lines.flatMap(l=>l.split(",")).map(x=>x.toString.toUpperCase)
    fmrdd.foreach(println)
    val lengths = lines.mapPartitions ( x => x.filter( l => l.length>20))
    lengths.foreach(println)
    val chennaiLines = lines.map(x=>x.split(",")).filter(y=>y(1).toUpperCase=="CHENNAI")
    chennaiLines.collect  //select vs collect(imp)
    chennaiLines.foreach(ar=>println(ar.mkString(",")))   
    val linesF1 = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
    val linesF2 = sc.textFile("file:/home/hduser/sparkdata/empdata1.txt")
    val unionOftwo = linesF1.union(linesF2)
    unionOftwo.foreach(println)
    println("DISTINCT")
    val unique = unionOftwo.distinct
    unique.foreach(println)
    println("bfr ZIP")
    val linezip = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
    linezip.foreach(println)
    println("aftr ZIP")
    val linezip1 = linezip.zip(linezip)
    linezip1.foreach(println)
    println("two files merge")
    val linezip2 = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
        val linezip3 = sc.textFile("file:/home/hduser/sparkdata/empdata1.txt")
    val linezip4 = linezip2.zip(linezip3)
    linezip4.foreach(println)
    println("GROUPBY")
    val customer =  lines.map(x=>x.split(",")).map(x=>Customer(x(0),x(1),x(2).toInt))
    customer.collect
    val grpbycity = customer.groupBy(x=>x.city)
    val grpbyage = customer.groupBy(x=>x.age)
    val grpbyname = customer.groupBy(x=>x.name)
    grpbycity.foreach(println)
    println("--------")
    grpbyage.foreach(println)
    println("--------")
    grpbyname.foreach(println)
    val number = sc.parallelize(1 to 10,4)
    println(number.partitions.size)
    println("bfr Coalesce")
    number.foreach(println)
    val coale = number.coalesce(3)
    println("after coalesce")
    //println(coale.partitions.size)
    coale.glom().collect
    coale.glom()  
    coale.foreach(println)
    println("after repartition")
    val repart = number.repartition(5)
    repart.foreach(println)
    println("SORTBY")
    val sortBy1 = customer.sortBy(p=>p.age,true)
    sortBy1.foreach(println)
    println("SORTBY-false")
    val sortBy2 = customer.sortBy(p=>p.age,false)
    sortBy2.foreach(println)
    println("checkpointing")
    sc.setCheckpointDir("/tmp/ckptdir") //local directory path
    // sc.setCheckpointDir("hdfs://localhost:54310/tmp/ckptdir") hdfs directory path
    val checkdir = sc.parallelize(1 to 10,4)
    checkdir.checkpoint()
    println(checkdir.count)
    println("Actions-")
    println("collect")
    val rd = sc.parallelize((1 to 100).toList)  
    val filterrd = rd filter(l=>(l%10)==0)
    val rdcollect = filterrd.collect
    rdcollect.foreach(println)
    val rdd = sc.parallelize(List(1,2,3,4,4,4,5,6,6,0,6,5,2,4,"00"))
    val count = rdd.countByValue()
    count.foreach(println)
    println("Reduce")
    val red = sc.parallelize(List(1,2,3,4,5))
    val sum = red.reduce((x,y)=>x+y)
    println(sum)
    val mul = red.reduce((x,y)=>x*y)
    println(mul)
    //val div = red.reduce((x,y)=>x/y)
    //println(div)
    println("CountBy")
    val cntby = sc.parallelize(List(("a",1),("b",2),("c",3),("c",4),("c",3)))
    val cnt = cntby.countByKey
    cnt.foreach(println)
    val cnt1 = cntby.countByValue
    cnt1.foreach(println)
    println("LookUP")
    val lookup1 = sc.parallelize(List(("a",1),("b",2),("c",3),("c",4),("c",3)))
    val lkup = lookup1.lookup("c")
    lkup.foreach(println)
//    val logs = sc.textFile("file:/usr/local/hadoop/logs/hadoop-hduser-datanode-Inceptez.log")
//    val errwrng = logs filter{l=>l.contains("ERROR") || l.contains("WARN")}
//    //errwrng.foreach(println)
//    val fs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI("hdfs://localhost:54310"),sc.hadoopConfiguration)
//    fs.delete(new org.apache.hadoop.fs.Path("/user/hduser/errwrng"),true)
 //   errwrng.saveAsTextFile("hdfs:///user/hduser/errwrng")
    println("saveAsTextFile")
    val logs = sc.textFile("file:/usr/local/hadoop/logs/hadoop-hduser-datanode-Inceptez.log")
    val errorsAndWarnings = logs filter { l => l.contains("ERROR") || l.contains("WARN")}
    val fs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI("hdfs://localhost:54310"),sc.hadoopConfiguration)
    fs.delete(new org.apache.hadoop.fs.Path("/user/hduser/errorsAndWarnings1"),true)
    errorsAndWarnings.saveAsTextFile("hdfs://localhost:54310/user/hduser/errorsAndWarnings1")
    println("Cache")
    errorsAndWarnings.cache()
    val err = errorsAndWarnings filter(l=>l.contains("ERROR"))
    val war = errorsAndWarnings filter(l=>l.contains("WARN"))
    println("ERROR: " + err.count)
    println("WARNING: " + war.count)
    println("PERSIST")
    errorsAndWarnings.persist()
    val line = sc.textFile("file:/home/hduser/sparkdata/empdata.txt")
    line.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
    line.unpersist()
    line.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY_2)
    line.unpersist()
    println("Broadcst variables")
    val input = sc.parallelize(List(1,2,4))
    val brdvar = sc.broadcast(2)
    val add = input.map(x=>brdvar.value+x)
    add.foreach(println)
  }
}